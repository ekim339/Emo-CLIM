{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths:\n",
    "ORIG_SPLIT_METADATA_FILES = {\n",
    "    # \"unbalanced_train\": \"/Users/eugenekim/Emo-CLIM/dataset/AudioSet/metadata_unbalanced_train.csv\",\n",
    "    \"balanced_train\": \"/Users/eugenekim/Emo-CLIM/dataset/AudioSet/metadata_balanced_train.csv\",\n",
    "    \"eval\": \"/Users/eugenekim/Emo-CLIM/dataset/AudioSet/metadata_eval.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script options:\n",
    "val_fract = 0.1\n",
    "test_fract = 0.1\n",
    "new_split_metadata_dir = \"/Users/eugenekim/Emo-CLIM/dataset/AudioSet/new_split_metadata_files\"\n",
    "new_split_metadata_files = {\n",
    "    \"train\": \"/Users/eugenekim/Emo-CLIM/dataset/AudioSet/new_split_metadata_files/metadata_train.csv\",\n",
    "    \"val\": \"/Users/eugenekim/Emo-CLIM/dataset/AudioSet/new_split_metadata_files/metadata_val.csv\",\n",
    "    \"test\": \"/Users/eugenekim/Emo-CLIM/dataset/AudioSet/new_split_metadata_files/metadata_test.csv\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading unbalanced_train set labels...\n",
      "Loading balanced_train set labels...\n",
      "Loading eval set labels...\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13713 entries, 0 to 13712\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   orig_subset     13713 non-null  object\n",
      " 1   file_name       13713 non-null  object\n",
      " 2   length_samples  13713 non-null  int64 \n",
      " 3   label           13713 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 428.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load original split metadata files:\n",
    "orig_split_metadata_dfs = {}\n",
    "for subset, file_path in ORIG_SPLIT_METADATA_FILES.items():\n",
    "    print(\"Loading {} set labels...\".format(subset))\n",
    "    orig_split_metadata_dfs[subset] = pd.read_csv(file_path)\n",
    "\n",
    "# concatenate original split labels into a single dataframe:\n",
    "all_metadata = pd.concat(orig_split_metadata_dfs.values(), axis=\"index\")\n",
    "all_metadata = all_metadata.reset_index(drop=True)\n",
    "print()\n",
    "print(all_metadata.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 80.00% of dataset = 10970 samples.\n",
      "Validation set size: 10.00% of dataset = 1371 samples.\n",
      "Test set size: 10.00% of dataset = 1371 samples.\n"
     ]
    }
   ],
   "source": [
    "# print subset sizes:\n",
    "train_set_size = int(np.around((1 - test_fract - val_fract) * all_metadata.shape[0]))\n",
    "print(\"Training set size: {:.2f}% of dataset = {} samples.\".format(100 * (1 - test_fract - val_fract), train_set_size))\n",
    "val_set_size = int(np.around(val_fract * all_metadata.shape[0]))\n",
    "print(\"Validation set size: {:.2f}% of dataset = {} samples.\".format(100 * val_fract, val_set_size))\n",
    "test_set_size = int(np.around(test_fract * all_metadata.shape[0]))\n",
    "print(\"Test set size: {:.2f}% of dataset = {} samples.\".format(100 * test_fract, test_set_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Exciting music clips: 4576\n",
      "Number of Tender music clips: 3358\n",
      "Number of Scary music clips: 1401\n",
      "Number of Sad music clips: 1385\n",
      "Number of Happy music clips: 1152\n",
      "Number of Angry music clips: 946\n",
      "Number of Funny music clips: 895\n"
     ]
    }
   ],
   "source": [
    "# get label counts:\n",
    "label_counts = all_metadata[\"label\"].value_counts()\n",
    "for label in all_metadata[\"label\"].value_counts().index:\n",
    "    print(\"Number of {} clips: {}\".format(label, label_counts[label]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1371 entries, 12030 to 9009\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   orig_subset     1371 non-null   object\n",
      " 1   file_name       1371 non-null   object\n",
      " 2   length_samples  1371 non-null   int64 \n",
      " 3   label           1371 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 53.6+ KB\n",
      "None\n",
      "\n",
      "Number of Exciting music clips: 458\n",
      "Number of Tender music clips: 336\n",
      "Number of Scary music clips: 140\n",
      "Number of Sad music clips: 138\n",
      "Number of Happy music clips: 115\n",
      "Number of Angry music clips: 95\n",
      "Number of Funny music clips: 89\n"
     ]
    }
   ],
   "source": [
    "# split into stratified training/val and test sets:\n",
    "metadata_train_val, metadata_test = train_test_split(all_metadata, test_size=test_set_size, stratify=all_metadata[\"label\"], random_state=42)\n",
    "assert metadata_test.shape[0] == test_set_size, \"Test set metadata has incorrect size.\"\n",
    "print(metadata_test.info())\n",
    "\n",
    "# print test set class distribution:\n",
    "print()\n",
    "label_counts = metadata_test[\"label\"].value_counts()\n",
    "for label in metadata_test[\"label\"].value_counts().index:\n",
    "    print(\"Number of {} clips: {}\".format(label, label_counts[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1371 entries, 7707 to 3650\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   orig_subset     1371 non-null   object\n",
      " 1   file_name       1371 non-null   object\n",
      " 2   length_samples  1371 non-null   int64 \n",
      " 3   label           1371 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 53.6+ KB\n",
      "None\n",
      "\n",
      "Number of Exciting music clips: 457\n",
      "Number of Tender music clips: 336\n",
      "Number of Scary music clips: 140\n",
      "Number of Sad music clips: 138\n",
      "Number of Happy music clips: 115\n",
      "Number of Angry music clips: 95\n",
      "Number of Funny music clips: 90\n"
     ]
    }
   ],
   "source": [
    "# split into stratified training and validation sets:\n",
    "metadata_train, metadata_val = train_test_split(metadata_train_val, test_size=val_set_size, stratify=metadata_train_val[\"label\"], random_state=42)\n",
    "assert metadata_val.shape[0] == val_set_size, \"Validation set metadata has incorrect size.\"\n",
    "print(metadata_val.info())\n",
    "\n",
    "# print validation set class distribution:\n",
    "print()\n",
    "label_counts = metadata_val[\"label\"].value_counts()\n",
    "for label in metadata_val[\"label\"].value_counts().index:\n",
    "    print(\"Number of {} clips: {}\".format(label, label_counts[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all subsets are disjoint:\n",
    "metadata_subsets = [metadata_train, metadata_val, metadata_test]\n",
    "subset_names = list(new_split_metadata_files.keys())\n",
    "for subset_1, name_1 in zip(metadata_subsets, subset_names):\n",
    "    for subset_2, name_2 in zip(metadata_subsets, subset_names):\n",
    "        if name_1 != name_2:\n",
    "            assert set(subset_1.index).isdisjoint(set(subset_2.index)), \"{} and {} are not disjoint\".format(name_1, name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset indices:\n",
    "metadata_train = metadata_train.reset_index(drop=True)\n",
    "metadata_val = metadata_val.reset_index(drop=True)\n",
    "metadata_test = metadata_test.reset_index(drop=True)\n",
    "\n",
    "# sanity checks:\n",
    "assert all_metadata.shape[0] == metadata_train.shape[0] + metadata_val.shape[0] + metadata_test.shape[0], \"Subset set sizes don't add up.\"\n",
    "# check that all subsets are disjoint:\n",
    "metadata_subsets = [metadata_train, metadata_val, metadata_test]\n",
    "subset_names = list(new_split_metadata_files.keys())\n",
    "for subset_1, name_1 in zip(metadata_subsets, subset_names):\n",
    "    for subset_2, name_2 in zip(metadata_subsets, subset_names):\n",
    "        if name_1 != name_2:\n",
    "            assert set(subset_1[\"file_name\"].tolist()).isdisjoint(set(subset_2[\"file_name\"].tolist())), \"{} and {} are not disjoint\".format(name_1, name_2)\n",
    "# more sanity checks:\n",
    "class_counts_all = all_metadata[\"label\"].value_counts()\n",
    "class_counts_train = metadata_train[\"label\"].value_counts()\n",
    "class_counts_val = metadata_val[\"label\"].value_counts()\n",
    "class_counts_test = metadata_test[\"label\"].value_counts()\n",
    "for class_label in all_metadata[\"label\"].unique().tolist():\n",
    "    assert class_counts_all[class_label] == class_counts_train[class_label] + class_counts_val[class_label] + class_counts_test[class_label], \"Error with splitting dataset.\"\n",
    "\n",
    "# save to file:\n",
    "metadata_train.to_csv(new_split_metadata_files[\"train\"], index=False)\n",
    "metadata_val.to_csv(new_split_metadata_files[\"val\"], index=False)\n",
    "metadata_test.to_csv(new_split_metadata_files[\"test\"], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-queries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
