{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths:\n",
    "ORIG_SPLIT_METADATA_FILES = {\n",
    "    \"unbalanced_train\": \"/proj/systewar/datasets/audioset_music_mood/metadata_unbalanced_train.csv\",\n",
    "    \"balanced_train\": \"/proj/systewar/datasets/audioset_music_mood/metadata_balanced_train.csv\",\n",
    "    \"eval\": \"/proj/systewar/datasets/audioset_music_mood/metadata_eval.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script options:\n",
    "N_class_val = 150     # number of samples per class in validation set\n",
    "N_class_test = 150     # number of samples per class in test set\n",
    "new_split_metadata_dir = \"/proj/systewar/datasets/audioset_music_mood/new_split_metadata_files\"\n",
    "new_split_metadata_files = {\n",
    "    \"train\": \"/proj/systewar/datasets/audioset_music_mood/new_split_metadata_files/metadata_train.csv\",\n",
    "    \"val\": \"/proj/systewar/datasets/audioset_music_mood/new_split_metadata_files/metadata_val.csv\",\n",
    "    \"test\": \"/proj/systewar/datasets/audioset_music_mood/new_split_metadata_files/metadata_test.csv\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading unbalanced_train set labels...\n",
      "Loading balanced_train set labels...\n",
      "Loading eval set labels...\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13713 entries, 0 to 13712\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   orig_subset     13713 non-null  object\n",
      " 1   file_name       13713 non-null  object\n",
      " 2   length_samples  13713 non-null  int64 \n",
      " 3   label           13713 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 428.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load original split metadata files:\n",
    "orig_split_metadata_dfs = {}\n",
    "for subset, file_path in ORIG_SPLIT_METADATA_FILES.items():\n",
    "    print(\"Loading {} set labels...\".format(subset))\n",
    "    orig_split_metadata_dfs[subset] = pd.read_csv(file_path)\n",
    "\n",
    "# concatenate original split labels into a single dataframe:\n",
    "all_metadata = pd.concat(orig_split_metadata_dfs.values(), axis=\"index\")\n",
    "all_metadata = all_metadata.reset_index(drop=True)\n",
    "print()\n",
    "print(all_metadata.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set size: 1050 samples = 7.66% of entire dataset.\n",
      "Test set size: 1050 samples = 7.66% of entire dataset.\n"
     ]
    }
   ],
   "source": [
    "# print validation and test sizes:\n",
    "n_classes = len(all_metadata[\"label\"].unique())\n",
    "val_set_size = N_class_val * n_classes\n",
    "print(\"Validation set size: {} samples = {:.2f}% of entire dataset.\".format(val_set_size, 100 * (val_set_size / all_metadata.shape[0])))\n",
    "test_set_size = N_class_test * n_classes\n",
    "print(\"Test set size: {} samples = {:.2f}% of entire dataset.\".format(test_set_size, 100 * (test_set_size / all_metadata.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Exciting music clips: 4576\n",
      "Number of Tender music clips: 3358\n",
      "Number of Scary music clips: 1401\n",
      "Number of Sad music clips: 1385\n",
      "Number of Happy music clips: 1152\n",
      "Number of Angry music clips: 946\n",
      "Number of Funny music clips: 895\n"
     ]
    }
   ],
   "source": [
    "# get label counts:\n",
    "label_counts = all_metadata[\"label\"].value_counts()\n",
    "for label in all_metadata[\"label\"].value_counts().index:\n",
    "    print(\"Number of {} clips: {}\".format(label, label_counts[label]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1050 entries, 5169 to 8349\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   orig_subset     1050 non-null   object\n",
      " 1   file_name       1050 non-null   object\n",
      " 2   length_samples  1050 non-null   int64 \n",
      " 3   label           1050 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 41.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# construct stratified test set by randomly sampling from each class:\n",
    "metadata_test_groups = all_metadata.groupby(by=\"label\", axis=\"index\")\n",
    "metadata_test = metadata_test_groups.sample(n=N_class_test, random_state=42)\n",
    "print(metadata_test.info())\n",
    "\n",
    "# sanity check:\n",
    "for count in metadata_test[\"label\"].value_counts():\n",
    "    assert count == N_class_test, \"Error with creating stratified test set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1050 entries, 12807 to 354\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   orig_subset     1050 non-null   object\n",
      " 1   file_name       1050 non-null   object\n",
      " 2   length_samples  1050 non-null   int64 \n",
      " 3   label           1050 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 41.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# remove test set:\n",
    "metadata_train_val = all_metadata.drop(index=list(metadata_test.index))\n",
    "\n",
    "# construct stratified validation set by randomly sampling from each class:\n",
    "metadata_val_groups = metadata_train_val.groupby(by=\"label\", axis=\"index\")\n",
    "metadata_val = metadata_val_groups.sample(n=N_class_val, random_state=42)\n",
    "print(metadata_val.info())\n",
    "\n",
    "# sanity check:\n",
    "for count in metadata_val[\"label\"].value_counts():\n",
    "    assert count == N_class_val, \"Error with creating stratified validation set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11613 entries, 0 to 13710\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   orig_subset     11613 non-null  object\n",
      " 1   file_name       11613 non-null  object\n",
      " 2   length_samples  11613 non-null  int64 \n",
      " 3   label           11613 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 453.6+ KB\n",
      "None\n",
      "\n",
      "Exciting music    4276\n",
      "Tender music      3058\n",
      "Scary music       1101\n",
      "Sad music         1085\n",
      "Happy music        852\n",
      "Angry music        646\n",
      "Funny music        595\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# construct training set:\n",
    "metadata_train = metadata_train_val.drop(index=list(metadata_val.index))\n",
    "print(metadata_train.info())\n",
    "print()\n",
    "print(metadata_train[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all subsets are disjoint:\n",
    "metadata_subsets = [metadata_train, metadata_val, metadata_test]\n",
    "subset_names = list(new_split_metadata_files.keys())\n",
    "for subset_1, name_1 in zip(metadata_subsets, subset_names):\n",
    "    for subset_2, name_2 in zip(metadata_subsets, subset_names):\n",
    "        if name_1 != name_2:\n",
    "            assert set(subset_1.index).isdisjoint(set(subset_2.index)), \"{} and {} are not disjoint\".format(name_1, name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset indices:\n",
    "metadata_train = metadata_train.reset_index(drop=True)\n",
    "metadata_val = metadata_val.reset_index(drop=True)\n",
    "metadata_test = metadata_test.reset_index(drop=True)\n",
    "\n",
    "# sanity checks:\n",
    "assert all_metadata.shape[0] == metadata_train.shape[0] + metadata_val.shape[0] + metadata_test.shape[0], \"Subset set sizes don't add up.\"\n",
    "# check that all subsets are disjoint:\n",
    "metadata_subsets = [metadata_train, metadata_val, metadata_test]\n",
    "subset_names = list(new_split_metadata_files.keys())\n",
    "for subset_1, name_1 in zip(metadata_subsets, subset_names):\n",
    "    for subset_2, name_2 in zip(metadata_subsets, subset_names):\n",
    "        if name_1 != name_2:\n",
    "            assert set(subset_1[\"file_name\"].tolist()).isdisjoint(set(subset_2[\"file_name\"].tolist())), \"{} and {} are not disjoint\".format(name_1, name_2)\n",
    "# more sanity checks:\n",
    "class_counts_all = all_metadata[\"label\"].value_counts()\n",
    "class_counts_train = metadata_train[\"label\"].value_counts()\n",
    "class_counts_val = metadata_val[\"label\"].value_counts()\n",
    "class_counts_test = metadata_test[\"label\"].value_counts()\n",
    "for class_label in metadata_test_groups.groups.keys():\n",
    "    assert class_counts_all[class_label] == class_counts_train[class_label] + class_counts_val[class_label] + class_counts_test[class_label], \"Error with splitting dataset.\"\n",
    "\n",
    "# save to file:\n",
    "metadata_train.to_csv(new_split_metadata_files[\"train\"], index=False)\n",
    "metadata_val.to_csv(new_split_metadata_files[\"val\"], index=False)\n",
    "metadata_test.to_csv(new_split_metadata_files[\"test\"], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-queries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
